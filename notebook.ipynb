{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning Llama-3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import threading\n",
    "import psutil\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "from util_fnx import b2mb, TorchTracemalloc, DataCollatorForInstructionTuning, IGNORE_INDEX\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import rnn\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, TaskType, get_peft_model\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds = load_dataset(\"kowndinya23/flan2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'cot' subset into train and test sets\n",
    "cot_train_test_split = ds['cot'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Create a new DatasetDict object for 'cot' with train and test splits\n",
    "cot_dataset = DatasetDict({\n",
    "    'train': cot_train_test_split['train'],\n",
    "    'test': cot_train_test_split['test']\n",
    "})\n",
    "\n",
    "# Update the original dataset dictionary to include the new DatasetDict for 'cot'\n",
    "ds['cot'] = cot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (15/15 shards): 100%|██████████| 4289888/4289888 [00:29<00:00, 144233.37 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 1072473/1072473 [00:06<00:00, 156708.12 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|██████████| 1320246/1320246 [00:09<00:00, 140141.95 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 330062/330062 [00:02<00:00, 138829.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 147078/147078 [00:00<00:00, 197763.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 36770/36770 [00:00<00:00, 181025.58 examples/s]\n",
      "Saving the dataset (21/21 shards): 100%|██████████| 8053516/8053516 [00:52<00:00, 153929.79 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 2013380/2013380 [00:12<00:00, 155264.08 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 443095/443095 [00:02<00:00, 164692.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 110774/110774 [00:00<00:00, 163850.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"kowndinya23/flan2022\")\n",
    "\n",
    "# Create a new DatasetDict to hold the train and test splits\n",
    "split_ds = DatasetDict()\n",
    "\n",
    "# Iterate over each subset in ds and split into train and test sets\n",
    "for subset in ds:\n",
    "    train_test_split = ds[subset].train_test_split(test_size=0.2)\n",
    "    split_ds[subset] = DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'test': train_test_split['test']\n",
    "    })\n",
    "\n",
    "# Save the split dataset to disk\n",
    "split_ds.save_to_disk('flan2022')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = DatasetDict.load_from_disk(f\"flan2022/flan2021\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "    num_rows: 4289888\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique task names: 70\n",
      "{'ai2_arc/ARC-Easy:1.0.0', 'aeslc:1.0.0', 'wmt16_translate/tr-en:1.0.0', 'wmt14_translate/fr-en:1.0.0', 'wmt16_translate/cs-en:1.0.0', 'natural_questions_open:1.0.0', 'lambada:1.0.0', 'super_glue/wic:1.0.2', 'huggingface:xsum', 'piqa:1.0.0', 'anli/r3:0.1.0', 'ai2_arc/ARC-Challenge:1.0.0', 'glue/qnli:2.0.0', 'samsum:1.0.0', 'gem/wiki_lingua_english_en:1.1.0', 'gem/dart:1.1.0', 'super_glue/wsc.fixed:1.0.2', 'word_segment', 'gem/e2e_nlg:1.1.0', 'glue/sst2:2.0.0', 'super_glue/cb:1.0.2', 'super_glue/copa:1.0.2', 'glue/wnli:2.0.0', 'glue/qqp:2.0.0', 'drop:2.0.0', 'coqa:1.0.0', 'paws_wiki:1.1.0', 'multi_news:1.0.0', 'wmt16_translate/ro-en:1.0.0', 'cnn_dailymail:3.4.0', 'story_cloze/2016:1.0.0', 'gigaword:1.2.0', 'ag_news_subset:1.0.0', 'snli:1.1.0', 'hellaswag:1.1.0', 'wmt16_translate/ru-en:1.0.0', 'trivia_qa/rc:1.1.0', 'definite_pronoun_resolution:1.1.0', 'wmt16_translate/fi-en:1.0.0', 'trec:1.0.0', 'unified_qa_science_inst', 'glue/stsb:2.0.0', 'super_glue/multirc:1.0.2', 'true_case', 'quac:1.0.0', 'bool_q:1.0.0', 'super_glue/record:1.0.2', 'openbookqa:0.1.0', 'gem/common_gen:1.1.0', 'imdb_reviews/plain_text:1.0.0', 'sentiment140:1.0.0', 'anli/r2:0.1.0', 'wmt16_translate/de-en:1.0.0', 'cosmos_qa:1.0.0', 'glue/cola:2.0.0', 'fix_punct', 'math_dataset/algebra__linear_1d:1.0.0', 'opinion_abstracts_rotten_tomatoes', 'squad/v2.0:3.0.0', 'super_glue/rte:1.0.2', 'yelp_polarity_reviews:0.2.0', 'squad/v1.1:3.0.0', 'anli/r1:0.1.0', 'winogrande:1.1.0', 'gem/web_nlg_en:1.1.0', 'glue/mnli:2.0.0', 'newsroom:1.0.0', 'opinion_abstracts_idebate', 'glue/mrpc:2.0.0', 'para_crawl_enes'}\n"
     ]
    }
   ],
   "source": [
    "unique_task_names = set(subset['task_name'])\n",
    "print(f\"Total number of unique task names: {len(unique_task_names)}\")\n",
    "print(unique_task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (15/15 shards): 100%|██████████| 4289888/4289888 [00:25<00:00, 171351.61 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 1072473/1072473 [00:06<00:00, 172117.08 examples/s]\n",
      "Saving the dataset (8/8 shards): 100%|██████████| 1320246/1320246 [00:08<00:00, 164318.24 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 330062/330062 [00:01<00:00, 172510.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 147078/147078 [00:00<00:00, 233681.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 36770/36770 [00:00<00:00, 234782.74 examples/s]\n",
      "Saving the dataset (21/21 shards): 100%|██████████| 8053516/8053516 [00:46<00:00, 174857.94 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 2013380/2013380 [00:11<00:00, 168334.92 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 443095/443095 [01:05<00:00, 6772.28 examples/s]  \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 110774/110774 [00:00<00:00, 201926.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds.save_to_disk('flan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (18/18 shards): 100%|██████████| 5362361/5362361 [00:06<00:00, 784431.61 examples/s]\n",
      "Saving the dataset (10/10 shards): 100%|██████████| 1650308/1650308 [00:03<00:00, 515223.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 183848/183848 [00:00<00:00, 910694.77 examples/s] \n",
      "Saving the dataset (27/27 shards): 100%|██████████| 10066896/10066896 [00:11<00:00, 866352.95 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 553869/553869 [00:01<00:00, 478013.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds.save_to_disk('flan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17817282"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5362361 + 1650308 + 183848 + 10066896 + 553869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    flan2021: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 5362361\n",
       "    })\n",
       "    t0: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 1650308\n",
       "    })\n",
       "    cot: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 183848\n",
       "    })\n",
       "    niv2: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 10066896\n",
       "    })\n",
       "    dialog: Dataset({\n",
       "        features: ['inputs', 'targets', 'task_source', 'task_name', 'template_type'],\n",
       "        num_rows: 553869\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'input ---- Just as with the color of your walls the color of your flooring also plays a significant role in the appearance of space\\noutput ---- Just as with the color of your walls, the color of your flooring also plays a significant role in the appearance of space.\\n\\n\\nAdd punctuation: 3821 But if you refuse to go forth this is the word that Yahweh has shown me\\nA: 38:21 But if you refuse to go forth, this is the word that Yahweh has shown me:\\n\\n\\nQUESTION: Fix punctuation: Later this month the German government will present its new energy outlook for 2050 with a key focus on the nuclear phaseout and the composition of the countrys future energy mix\\nANS: Later this month, the German government will present its new “energy outlook for 2050,” with a key focus on the nuclear phaseout and the composition of the country’s future energy mix.\\n\\n\\nQUES: Your vision of life may be more universal and you may be drawn to spiritual or esoteric subjects which previously you might have overlooked underrated or simply rejected because they seemed irrational\\n\\ncorrect the punctuation.\\n\\nCORRECTED: Your vision of life may be more universal, and you may be drawn to spiritual or esoteric subjects which previously you might have overlooked, underrated, or simply rejected because they seemed \"irrational\".\\n\\n\\nQuestion: We say thanks for a wonderful trip with sweet students and good spirit even on a cold night in the tents at Rgen\\n--\\nAnswer: We say thanks for a wonderful trip with sweet students and good spirit even on a cold night in the tents at Rügen.\\n\\n\\ninput: On 8 April of60 was born in Aricebo In Puerto Rico the SETI acronym for Search for ExtraTerrestrial Intelligence a research center whose goal is to pick radio signals from other planets  The only result was obtained on the first day a kind of Regular cry coming from the star Epsilon Eridani  This star is 24 billion miles away from us there are those who say that it is almost impossible that quellurlo be directed to us however there are those who say that directly to us or not that day on Epsilon Eridani something was happening \\nfixed:',\n",
       " 'targets': 'On 8 April of\\'60 was born in Aricebo In Puerto Rico, the SETI (acronym for \"Search for Extra-Terrestrial Intelligence), a research center whose goal is to pick radio signals from other planets . The only result was obtained on the first day: a kind of Regular cry coming from the star Epsilon Eridani . This star is 24 billion miles away from us, there are those who say that it is almost impossible that quell\\'urlo be directed to us, however there are those who say that directly to us or not that day on Epsilon Eridani something was happening ...',\n",
       " 'task_source': 'Flan2021',\n",
       " 'task_name': 'fix_punct',\n",
       " 'template_type': 'fs_opt'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['flan2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique template types: 4\n"
     ]
    }
   ],
   "source": [
    "# Count the unique template types\n",
    "unique_template_types = set(item['template_type'] for item in ds['flan2021'])\n",
    "total_unique_template_types = len(unique_template_types)\n",
    "\n",
    "print(f\"Total number of unique template types: {total_unique_template_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fs_noopt', 'fs_opt', 'zs_noopt', 'zs_opt'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_template_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds['flan2021'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>task_source</th>\n",
       "      <th>task_name</th>\n",
       "      <th>template_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5362361</td>\n",
       "      <td>5362361</td>\n",
       "      <td>5362361</td>\n",
       "      <td>5362361</td>\n",
       "      <td>5362361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4211385</td>\n",
       "      <td>1347467</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Write a sentence not in English.</td>\n",
       "      <td>no</td>\n",
       "      <td>Flan2021</td>\n",
       "      <td>glue/mnli:2.0.0</td>\n",
       "      <td>zs_opt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>37707</td>\n",
       "      <td>272296</td>\n",
       "      <td>5362361</td>\n",
       "      <td>216560</td>\n",
       "      <td>1341883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  inputs  targets task_source  \\\n",
       "count                            5362361  5362361     5362361   \n",
       "unique                           4211385  1347467           1   \n",
       "top     Write a sentence not in English.       no    Flan2021   \n",
       "freq                               37707   272296     5362361   \n",
       "\n",
       "              task_name template_type  \n",
       "count           5362361       5362361  \n",
       "unique               70             4  \n",
       "top     glue/mnli:2.0.0        zs_opt  \n",
       "freq             216560       1341883  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_DTYPES={\n",
    "    \"float32\": torch.float32,\n",
    "    \"float16\": torch.float16,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"auto\": \"auto\"\n",
    "}\n",
    "torch_dtype = 'auto' #default\n",
    "torch_dtype=TORCH_DTYPES[torch_dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir, \n",
    "    token=hf_access_token\n",
    ")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side=\"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configurations\n",
    "base_model=AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    cache_dir=cache_dir,\n",
    "    token=hf_access_token,\n",
    "    use_flash_attention_2=use_flash_attention_2\n",
    ")\n",
    "base_model.config.use_cache=False\n",
    "base_model.config.sliding_window=sliding_window\n",
    "\n",
    "embedding_size=base_model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer)>embedding_size:\n",
    "    base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one log on every process with the configuration for debugging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "# logger.info(accelerator.state, main_process_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Define variables to replace args.<variable>\n",
    "push_to_hub = True\n",
    "hub_model_id = None  # Replace with model ID or set to None to infer from output_dir\n",
    "output_dir = \"/path/to/output\"\n",
    "private_repo = True\n",
    "hub_token = \"your_hub_token_here\"\n",
    "\n",
    "if push_to_hub:\n",
    "    # Retrieve or infer repo_name\n",
    "    repo_name = hub_model_id if hub_model_id is not None else Path(output_dir).absolute().name\n",
    "    \n",
    "    # Create repo and retrieve repo_id\n",
    "    repo_id = create_repo(repo_name, exist_ok=True, token=hub_token, private=private_repo).repo_id\n",
    "    \n",
    "    # Clone repo locally\n",
    "    repo = Repository(output_dir, clone_from=repo_id, token=hub_token)\n",
    "\n",
    "    # Set up .gitignore for specific patterns\n",
    "    gitignore_path = os.path.join(output_dir, \".gitignore\")\n",
    "    with open(gitignore_path, \"w+\") as gitignore:\n",
    "        if \"step_*\" not in gitignore:\n",
    "            gitignore.write(\"step_*\\n\")\n",
    "        if \"epoch_*\" not in gitignore:\n",
    "            gitignore.write(\"epoch_*\\n\")\n",
    "elif output_dir is not None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data_path = \"\"\n",
    "raw_dataset = load_dataset(path=data_path)\n",
    "# Preprocessing the datasets\n",
    "raw_dataset_column_names=raw_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We preprocess the data in THREE steps:\n",
    "   1. Concatenate prompts and responses\n",
    "   2. Tokenize the concatenated prompt-response pairs\n",
    "   3. Set the labels corresponding to the prompt tokens to IGNORE_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    prompts_responses=[p+\" \"+r for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
    "    prompts_responses_tokenized=tokenizer(prompts_responses, truncation=True, max_length=max_seq_length)\n",
    "    prompts_tokenized=tokenizer(examples[\"prompt\"], truncation=True, max_length=max_seq_length)\n",
    "    all_labels=copy.deepcopy(prompts_responses_tokenized[\"input_ids\"])\n",
    "    prompts_len=[len(prompt) for prompt in prompts_tokenized[\"input_ids\"]]\n",
    "    for labels, prompt_len in zip(all_labels, prompts_len):\n",
    "        labels[:prompt_len]=[IGNORE_INDEX]*prompt_len\n",
    "    result={k: v for k, v in prompts_responses_tokenized.items()}\n",
    "    result[\"labels\"]=all_labels\n",
    "    return result\n",
    "\n",
    "preprocessed_dataset=raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    "    remove_columns=raw_dataset_column_names,\n",
    "    desc=\"Preprocessing the raw dataset\",\n",
    ")\n",
    "\n",
    "train_dataset=preprocessed_dataset[\"train\"]\n",
    "eval_dataset=preprocessed_dataset[\"validation\"]\n",
    "\n",
    "# DataLoaders creation\n",
    "data_collator=DataCollatorForInstructionTuning(tokenizer)\n",
    "train_dataloader=DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size, pin_memory=True, num_workers=8\n",
    ")\n",
    "eval_dataloader=DataLoader(\n",
    "    eval_dataset, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size, pin_memory=True, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration if --use_peft is passed\n",
    "if args.use_peft:\n",
    "    peft_config=LoraConfig(\n",
    "        r=args.peft_lora_r,\n",
    "        lora_alpha=args.peft_lora_alpha,\n",
    "        lora_dropout=args.peft_lora_dropout,\n",
    "        target_modules=args.peft_target_modules.split(\",\"),\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model=get_peft_model(base_model, peft_config)\n",
    "else:\n",
    "    model=base_model\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(params=model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay, fused=args.adamw_fused)\n",
    "\n",
    "# Scheduler and math around the number of training steps\n",
    "overrode_max_train_steps=False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler=get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=math.floor(args.lr_warmup_fraction*args.max_train_steps),\n",
    "    num_training_steps=args.max_train_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def b2mb(x): \n",
    "    return x / 2**20\n",
    "\n",
    "class TorchTracemalloc:\n",
    "    def __init__(self):\n",
    "        self.begin = 0\n",
    "        self.peaked = 0\n",
    "        self.used = 0\n",
    "        self.cpu_begin = 0\n",
    "        self.cpu_peaked = 0\n",
    "        self.cpu_used = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        self.cpu_begin = torch.cuda.memory_reserved()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.used = torch.cuda.memory_allocated() - self.begin\n",
    "        self.peaked = torch.cuda.max_memory_allocated() - self.begin\n",
    "        self.cpu_used = torch.cuda.memory_reserved() - self.cpu_begin\n",
    "        self.cpu_peaked = torch.cuda.max_memory_reserved() - self.cpu_begin\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        model.train()\n",
    "        total_loss = 0 if args.with_tracking else None\n",
    "        \n",
    "        # Handle checkpoint resumption\n",
    "        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "            for _ in range(resume_step):\n",
    "                next(iter(train_dataloader))\n",
    "                \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if args.with_tracking:\n",
    "                total_loss += loss.detach().float()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if args.with_tracking:\n",
    "                    logger.log({\n",
    "                        \"instant_loss\": loss.item() * args.gradient_accumulation_steps,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"step\": completed_steps\n",
    "                    })\n",
    "                \n",
    "                completed_steps += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if isinstance(checkpointing_steps, int) and completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                    }, output_dir)\n",
    "            \n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "    \n",
    "    # Print memory usage for training\n",
    "    print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\n",
    "    print(f\"GPU Memory consumed at the end of the train: {b2mb(tracemalloc.used)}\")\n",
    "    print(f\"GPU Peak Memory consumed during the train: {b2mb(tracemalloc.peaked)}\")\n",
    "    print(f\"GPU Total Peak Memory consumed during the train: {b2mb(tracemalloc.peaked + tracemalloc.begin)}\")\n",
    "    \n",
    "    print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    print(f\"CPU Memory consumed at the end of the train: {b2mb(tracemalloc.cpu_used)}\")\n",
    "    print(f\"CPU Peak Memory consumed during the train: {b2mb(tracemalloc.cpu_peaked)}\")\n",
    "    print(f\"CPU Total Peak Memory consumed during the train: {b2mb(tracemalloc.cpu_peaked + tracemalloc.cpu_begin)}\")\n",
    "            \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                losses.extend([loss] * args.per_device_eval_batch_size)\n",
    "\n",
    "    # Print memory usage for evaluation\n",
    "    print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\n",
    "    print(f\"GPU Memory consumed at the end of the eval: {b2mb(tracemalloc.used)}\")\n",
    "    print(f\"GPU Peak Memory consumed during the eval: {b2mb(tracemalloc.peaked)}\")\n",
    "    print(f\"GPU Total Peak Memory consumed during the eval: {b2mb(tracemalloc.peaked + tracemalloc.begin)}\")\n",
    "    \n",
    "    print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    print(f\"CPU Memory consumed at the end of the eval: {b2mb(tracemalloc.cpu_used)}\")\n",
    "    print(f\"CPU Peak Memory consumed during the eval: {b2mb(tracemalloc.cpu_peaked)}\")\n",
    "    print(f\"CPU Total Peak Memory consumed during the eval: {b2mb(tracemalloc.cpu_peaked + tracemalloc.cpu_begin)}\")\n",
    "\n",
    "    try:\n",
    "        eval_loss = torch.mean(torch.tensor(losses))\n",
    "        perplexity = math.exp(eval_loss.item())\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "    if args.with_tracking:\n",
    "        logger.log({\n",
    "            \"perplexity\": perplexity,\n",
    "            \"eval_loss\": eval_loss.item(),\n",
    "            \"train_loss\": total_loss.item() / len(train_dataloader) if total_loss is not None else None,\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": completed_steps,\n",
    "        })\n",
    "    \n",
    "    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        model.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\",\n",
    "            blocking=False,\n",
    "            auto_lfs_prune=True\n",
    "        )\n",
    "    \n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters as variables\n",
    "starting_epoch = 0\n",
    "num_train_epochs = 10  # Example value\n",
    "with_tracking = True\n",
    "resume_from_checkpoint = False\n",
    "gradient_accumulation_steps = 4\n",
    "output_dir = \"/path/to/output\"\n",
    "max_train_steps = 1000\n",
    "checkpointing_steps = \"epoch\"\n",
    "per_device_eval_batch_size = 8\n",
    "push_to_hub = False\n",
    "\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        model.train()\n",
    "        total_loss = 0 if with_tracking else None\n",
    "\n",
    "        # Handle checkpoint resumption\n",
    "        # if resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "        #     for _ in range(resume_step):\n",
    "        #         next(iter(train_dataloader))\n",
    "                \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            if with_tracking:\n",
    "                total_loss += loss.detach().float()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if with_tracking:\n",
    "                    logger.log({\n",
    "                        \"instant_loss\": loss.item() * gradient_accumulation_steps,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"step\": completed_steps\n",
    "                    })\n",
    "                \n",
    "                completed_steps += 1\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if isinstance(checkpointing_steps, int) and completed_steps % checkpointing_steps == 0:\n",
    "                    step_output_dir = f\"step_{completed_steps}\"\n",
    "                    if output_dir is not None:\n",
    "                        step_output_dir = os.path.join(output_dir, step_output_dir)\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                    }, step_output_dir)\n",
    "            \n",
    "            if completed_steps >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    # Print memory usage for training\n",
    "    print(f\"GPU Memory before entering the train : {b2mb(tracemalloc.begin)}\")\n",
    "    print(f\"GPU Memory consumed at the end of the train: {b2mb(tracemalloc.used)}\")\n",
    "    print(f\"GPU Peak Memory consumed during the train: {b2mb(tracemalloc.peaked)}\")\n",
    "    print(f\"GPU Total Peak Memory consumed during the train: {b2mb(tracemalloc.peaked + tracemalloc.begin)}\")\n",
    "    \n",
    "    print(f\"CPU Memory before entering the train : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    print(f\"CPU Memory consumed at the end of the train: {b2mb(tracemalloc.cpu_used)}\")\n",
    "    print(f\"CPU Peak Memory consumed during the train: {b2mb(tracemalloc.cpu_peaked)}\")\n",
    "    print(f\"CPU Total Peak Memory consumed during the train: {b2mb(tracemalloc.cpu_peaked + tracemalloc.cpu_begin)}\")\n",
    "            \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with TorchTracemalloc() as tracemalloc:\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                losses.extend([loss] * per_device_eval_batch_size)\n",
    "\n",
    "    # Print memory usage for evaluation\n",
    "    print(f\"GPU Memory before entering the eval : {b2mb(tracemalloc.begin)}\")\n",
    "    print(f\"GPU Memory consumed at the end of the eval: {b2mb(tracemalloc.used)}\")\n",
    "    print(f\"GPU Peak Memory consumed during the eval: {b2mb(tracemalloc.peaked)}\")\n",
    "    print(f\"GPU Total Peak Memory consumed during the eval: {b2mb(tracemalloc.peaked + tracemalloc.begin)}\")\n",
    "    \n",
    "    print(f\"CPU Memory before entering the eval : {b2mb(tracemalloc.cpu_begin)}\")\n",
    "    print(f\"CPU Memory consumed at the end of the eval: {b2mb(tracemalloc.cpu_used)}\")\n",
    "    print(f\"CPU Peak Memory consumed during the eval: {b2mb(tracemalloc.cpu_peaked)}\")\n",
    "    print(f\"CPU Total Peak Memory consumed during the eval: {b2mb(tracemalloc.cpu_peaked + tracemalloc.cpu_begin)}\")\n",
    "\n",
    "    try:\n",
    "        eval_loss = torch.mean(torch.tensor(losses))\n",
    "        perplexity = math.exp(eval_loss.item())\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "    if with_tracking:\n",
    "        logger.log({\n",
    "            \"perplexity\": perplexity,\n",
    "            \"eval_loss\": eval_loss.item(),\n",
    "            \"train_loss\": total_loss.item() / len(train_dataloader) if total_loss is not None else None,\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": completed_steps,\n",
    "        })\n",
    "    \n",
    "    if push_to_hub and epoch < num_train_epochs - 1:\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\",\n",
    "            blocking=False,\n",
    "            auto_lfs_prune=True\n",
    "        )\n",
    "    \n",
    "    if checkpointing_steps == \"epoch\":\n",
    "        epoch_output_dir = f\"epoch_{epoch}\"\n",
    "        if output_dir is not None:\n",
    "            epoch_output_dir = os.path.join(output_dir, epoch_output_dir)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, epoch_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Define variables to replace args.<variable>\n",
    "with_tracking = True\n",
    "output_dir = \"/path/to/output\"\n",
    "push_to_hub = False\n",
    "use_peft = False\n",
    "merge_weights = False\n",
    "\n",
    "# End tracking if specified\n",
    "if with_tracking:\n",
    "    # Logic to end tracking (use wandb, tensorboard, etc. if needed)\n",
    "    print(\"End of tracking\")\n",
    "\n",
    "# Save model and tokenizer if output directory is provided\n",
    "if output_dir is not None:\n",
    "    # Save the model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # If pushing to a hub\n",
    "    if push_to_hub:\n",
    "        # Replace with the hub-specific code for pushing the repo\n",
    "        repo.push_to_hub(commit_message=\"End of Training\", auto_lfs_prune=True)\n",
    "\n",
    "    # Save final results in JSON\n",
    "    with open(os.path.join(output_dir, \"all_results.json\"), \"w\") as f:\n",
    "        json.dump({\"perplexity\": perplexity}, f)\n",
    "\n",
    "# Merge weights if using PEFT and merging is specified\n",
    "if use_peft and merge_weights:\n",
    "    # Free memory for merging weights\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load and merge model weights\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch_dtype)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Save the merged model to a separate directory\n",
    "    output_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\n",
    "    model.save_pretrained(output_merged_dir)\n",
    "    tokenizer.save_pretrained(output_merged_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_flan2022():\n",
    "    print(\"Check if FLAN 2022 dataset is already downloaded. If not, download it and load it\")\n",
    "    for submixture in SUBMIXTURES:\n",
    "        print(f\"Loading {submixture} dataset...\")\n",
    "        dataset=load_dataset(f\"{HUB_USERNAME}/{submixture}-submix-4096\")\n",
    "        print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
